{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BatchExtraction 脚本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function：根据phase_gen提供不同的batch_extraction策略  \n",
    "1⃣️ phase_gen == \"test\"：    \n",
    "    input:测试视频的路径\n",
    "    对视频逐帧提取frame以便送入网络  \n",
    "2⃣️ phase_gen == \"train\":  \n",
    "    input:训练视频的路径  \n",
    "    video_train_paths里包含所有视频编号的video  \n",
    "    每一个video里有三个文件夹   \n",
    "    “image”:该视频的所有视频帧、\"maps\":该视频的连续的热度图、 \"fixation/maps/\":该视频离散的热度图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.ndimage\n",
    "from scipy.misc import imread, imresize\n",
    "\n",
    "def padding(img, shape_r=240, shape_c=320, channels=3):\n",
    "    img_padded = np.zeros((shape_r, shape_c, channels), dtype=np.uint8)\n",
    "    if channels == 1:\n",
    "        img_padded = np.zeros((shape_r, shape_c), dtype=np.uint8)\n",
    "\n",
    "    original_shape = img.shape\n",
    "    rows_rate = original_shape[0]/shape_r\n",
    "    cols_rate = original_shape[1]/shape_c\n",
    "\n",
    "    if rows_rate > cols_rate:\n",
    "        new_cols = (original_shape[1] * shape_r) // original_shape[0]\n",
    "        img = imresize(img, (shape_r, new_cols))\n",
    "        if new_cols > shape_c:\n",
    "            new_cols = shape_c\n",
    "        img_padded[:, ((img_padded.shape[1] - new_cols) // 2):((img_padded.shape[1] - new_cols) // 2 + new_cols),] = img\n",
    "    else:\n",
    "        new_rows = (original_shape[0] * shape_c) // original_shape[1]\n",
    "        img = imresize(img, (new_rows,shape_c))\n",
    "        if new_rows > shape_r:\n",
    "            new_rows = shape_r\n",
    "        img_padded[((img_padded.shape[0] - new_rows) // 2):((img_padded.shape[0] - new_rows) // 2 + new_rows), :] = img\n",
    "\n",
    "    return img_padded\n",
    "\n",
    "\n",
    "def resize_fixation(img, rows=480, cols=640):\n",
    "    out = np.zeros((rows, cols))\n",
    "    factor_scale_r = rows / img.shape[0]\n",
    "    factor_scale_c = cols / img.shape[1]\n",
    "\n",
    "    coords = np.argwhere(img)\n",
    "    for coord in coords:\n",
    "        r = int(np.round(coord[0]*factor_scale_r))\n",
    "        c = int(np.round(coord[1]*factor_scale_c))\n",
    "        if r == rows:\n",
    "            r -= 1\n",
    "        if c == cols:\n",
    "            c -= 1\n",
    "        out[r, c] = 1\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def padding_fixation(img, shape_r=480, shape_c=640):\n",
    "    img_padded = np.zeros((shape_r, shape_c))\n",
    "\n",
    "    original_shape = img.shape\n",
    "    rows_rate = original_shape[0]/shape_r\n",
    "    cols_rate = original_shape[1]/shape_c\n",
    "\n",
    "    if rows_rate > cols_rate:\n",
    "        new_cols = (original_shape[1] * shape_r) // original_shape[0]\n",
    "        img = resize_fixation(img, rows=shape_r, cols=new_cols)\n",
    "        if new_cols > shape_c:\n",
    "            new_cols = shape_c\n",
    "        img_padded[:, ((img_padded.shape[1] - new_cols) // 2):((img_padded.shape[1] - new_cols) // 2 + new_cols),] = img\n",
    "    else:\n",
    "        new_rows = (original_shape[0] * shape_c) // original_shape[1]\n",
    "        img = resize_fixation(img, rows=new_rows, cols=shape_c)\n",
    "        if new_rows > shape_r:\n",
    "            new_rows = shape_r\n",
    "        img_padded[((img_padded.shape[0] - new_rows) // 2):((img_padded.shape[0] - new_rows) // 2 + new_rows), :] = img\n",
    "\n",
    "    return img_padded\n",
    "\n",
    "\n",
    "def preprocess_images(paths, shape_r, shape_c):\n",
    "    ims = np.zeros((len(paths), shape_r, shape_c, 3))\n",
    "\n",
    "    for i, path in enumerate(paths):\n",
    "        # original_image = cv2.imread(path)\n",
    "        # original_image = mpimg.imread(path)\n",
    "        original_image = imread(path)\n",
    "        if original_image.ndim == 2:\n",
    "            copy = np.zeros((original_image.shape[0], original_image.shape[1], 3))\n",
    "            copy[:, :, 0] = original_image\n",
    "            copy[:, :, 1] = original_image\n",
    "            copy[:, :, 2] = original_image\n",
    "            original_image = copy\n",
    "        padded_image = padding(original_image, shape_r, shape_c, 3)\n",
    "        ims[i] = padded_image\n",
    "\n",
    "# 3 个通道为什么要减数字做偏差？\n",
    "    ims[:, :, :, 0] -= 103.939\n",
    "    ims[:, :, :, 1] -= 116.779\n",
    "    ims[:, :, :, 2] -= 123.68\n",
    "    ims = ims[:, :, :, ::-1]\n",
    "    # ims = ims.transpose((0, 3, 1, 2))\n",
    "\n",
    "    return ims\n",
    "\n",
    "\n",
    "def preprocess_maps(paths, shape_r, shape_c):\n",
    "    ims = np.zeros((len(paths), shape_r, shape_c, 1))\n",
    "\n",
    "    for i, path in enumerate(paths):\n",
    "        # original_map = cv2.imread(path, 0)\n",
    "        # original_map = mpimg.imread(path)\n",
    "        original_map = imread(path)\n",
    "        padded_map = padding(original_map, shape_r, shape_c, 1)\n",
    "        ims[i, :, :, 0] = padded_map.astype(np.float32)\n",
    "        ims[i, :, :, 0] /= 255.0\n",
    "\n",
    "    return ims\n",
    "\n",
    "\n",
    "def preprocess_fixmaps(paths, shape_r, shape_c):\n",
    "    ims = np.zeros((len(paths), shape_r, shape_c, 1))\n",
    "\n",
    "    for i, path in enumerate(paths):\n",
    "        fix_map = scipy.io.loadmat(path)[\"I\"]\n",
    "        ims[i, :, :, 0] = padding_fixation(fix_map, shape_r=shape_r, shape_c=shape_c)\n",
    "\n",
    "    return ims\n",
    "\n",
    "\n",
    "def postprocess_predictions(pred, shape_r, shape_c):\n",
    "    predictions_shape = pred.shape\n",
    "    rows_rate = shape_r / predictions_shape[0]\n",
    "    cols_rate = shape_c / predictions_shape[1]\n",
    "\n",
    "    pred = pred / np.max(pred) * 255\n",
    "\n",
    "    if rows_rate > cols_rate:\n",
    "        new_cols = (predictions_shape[1] * shape_r) // predictions_shape[0]\n",
    "        # pred = cv2.resize(pred, (new_cols, shape_r))\n",
    "        pred = imresize(pred, (shape_r, new_cols))\n",
    "        img = pred[:, ((pred.shape[1] - shape_c) // 2):((pred.shape[1] - shape_c) // 2 + shape_c)]\n",
    "    else:\n",
    "        new_rows = (predictions_shape[0] * shape_c) // predictions_shape[1]\n",
    "        # pred = cv2.resize(pred, (shape_c, new_rows))\n",
    "        pred = imresize(pred, (new_rows, shape_c))\n",
    "        img = pred[((pred.shape[0] - shape_r) // 2):((pred.shape[0] - shape_r) // 2 + shape_r),:]\n",
    "\n",
    "    img = scipy.ndimage.filters.gaussian_filter(img, sigma=7)\n",
    "    img = img / np.max(img) * 255\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "VideoName= \"/Users/yanhang/Downloads/animal_alpaca01.mp4\"\n",
    "Video_train_paths = \"/Users/yanhang/Desktop/saliency/my_models/videotrain\"\n",
    "# path of training maps\n",
    "maps_path = '/maps/'\n",
    "# path of training fixation maps\n",
    "fixs_path = '/fixation/maps/'\n",
    "# path of training video frames\n",
    "frames_path = '/images/'\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import scipy.io\n",
    "import scipy.ndimage\n",
    "from scipy.misc import imread, imresize\n",
    "\n",
    "\n",
    "#显著度视频的batch提取，batch_shape: (batch_size, frame_num, width, height, RGB)\n",
    "#对一个视频随机抽取一段连续帧为frame_num的一组实例\n",
    "def _VideoBatch_Generator(VideoName, batch_size, frame_num, input_shape, output_shape, phase_gen='train'):\n",
    "    '''\n",
    "    Args:\n",
    "    VideoCap: cv2预读取视频的操作\n",
    "    batch_size: batch的大小\n",
    "    frame_num: 一个视频的连续帧\n",
    "    resize_shape: 输出视频帧的大小\n",
    "    Out: video_batch: (batch_size, frame_num, width, height, RGB)\n",
    "    '''\n",
    "    VideoCap = cv2.VideoCapture(VideoName)\n",
    "    \n",
    "    #frame_width = VideoCap.get(3)  #3:CV_CAP_PROP_FRAME_HEIGHT\n",
    "    #frame_height = VideoCap.get(4) #4:CV_CAP_PROP_FRAME_HEIGHT\n",
    "    if phase_gen == 'test':\n",
    "        Video_frames = VideoCap.get(7)  #7:CV_CAP_PROP_FRAME_COUNT\n",
    "        print(Video_frames)\n",
    "        \n",
    "        batch_size = Video_frames // frame_num\n",
    "        if not Video_frames % frame_num == 0:\n",
    "            batch_size = batch_size + 1\n",
    "        print(batch_size)\n",
    "        \n",
    "        start_frame = 0\n",
    "        Video_Batch = np.zeros(shape=[1, 1, 720, 1080, 3])\n",
    "        \n",
    "        \n",
    "        for j in range(int(batch_size)):\n",
    "            Video_Slice = np.zeros(shape=[1, 1, 720, 1080, 3])\n",
    "            if start_frame + frame_num > Video_frames:\n",
    "                start_frame = Video_frames - frame_num\n",
    "            stop_frame = start_frame + frame_num \n",
    "            for i in range(start_frame, stop_frame):\n",
    "                VideoCap.set(1, i)          #1:CV_CAP_PROP_POS_FRAMES\n",
    "                _, frame = VideoCap.read()   #（frame_width, frame_height, BGR）\n",
    "                frame = cv2.resize(frame, resize_shape) \n",
    "                frame = frame.astype(np.float32)\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)#（resize_width, resize_height, RGB）\n",
    "                frame = frame[np.newaxis, np.newaxis, ...]\n",
    "                if np.shape(Video_Slice)[1] == 1 and i == start_frame:\n",
    "                    Video_Slice = frame\n",
    "                else:\n",
    "                    Video_Slice = np.concatenate((Video_Slice, frame), axis=1)                \n",
    "                print(np.shape(Video_Slice))\n",
    "            if np.shape(Video_Batch)[0] == 1 and start_frame == 0:\n",
    "                Video_Batch = Video_Slice\n",
    "            else:\n",
    "                Video_Batch = np.concatenate((Video_Batch, Video_Slice), axis=0)\n",
    "            print(np.shape(Video_Batch))\n",
    "            start_frame = stop_frame\n",
    "    #training process 读取的是文件夹里的照片\n",
    "    # video_trainpaths里包含的是按数字标号保存的video，每个video文件家里三个文件夹 “images” “maps” \"dixations\"\n",
    "    if phase_gen == 'train':\n",
    "        videos = [Video_train_paths + '/' + video_train_path for video_train_path \n",
    "                  in os.listdir(Video_train_paths) if os.path.isdir(Video_train_paths + '/' + video_train_path)]\n",
    "        videos.sort()\n",
    "        random.shuffle(videos)\n",
    "        \n",
    "        range_num = 0\n",
    "        while True:\n",
    "            Xims = np.zeros((video_b_s, num_frames, resize_shape[0], resize_shape[1], 3))\n",
    "\n",
    "            Ymaps = np.zeros((video_b_s, num_frames, output_shape[0], output_shape[1], 1)) + 0.01\n",
    "            Yfixs = np.zeros((video_b_s, num_frames, output_shape[0], output_shape[1], 1)) + 0.01\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                video = videos[(range_num * batch_size + i) % len(videos)]\n",
    "                images = [video + frames_path + f for f in os.listdir(video + frames_path) if f.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "                maps = [video + maps_path + f for f in os.listdir(video + maps_path) if f.endswith((\".jpg\", \".jpeg\", \"png\"))]\n",
    "                fixs = [video + fixs_path + f for f in os.listdir(video + fixs_path) if f.endswith(\".mat\")]\n",
    "                \n",
    "                start = np.random.choice(max(1, len(images) - frame_num))\n",
    "                X = preprocess_images(images[start:min((start + frame_num), len(images))], resize_shape[0], resize_shape[1])\n",
    "                Y = preprocess_maps(maps[start:min((start + frame_num), len(images))], resize_shape[0], resize_shape[1])\n",
    "                Y_fix = preprocess_fixs(fixs[start:min((start + frame_num), len(images))], resize_shape[0], resize_shape[1])\n",
    "                \n",
    "                Xims[i, 0:np.shape(X)[0], ...] = np.copy(X)\n",
    "                Ymaps[i, 0:np.shape(Y)[0], ...] = np.copy(Y)\n",
    "                Yfixs[i, 0:np.shape(Y_fix)[0], ...] = np.copy(Y_fix)\n",
    "                \n",
    "                Xims[i, X.shape[0]:num_frames, :] = np.copy(X[-1, :, :])\n",
    "                Ymaps[i, Y.shape[0]:num_frames, :] = np.copy(Y[-1, :, :])\n",
    "                Yfixs[i, Y_fix.shape[0]:num_frames, :] = np.copy(Y_fix[-1, :, :])\n",
    "            yield [Xims, Ymaps, Yfixs]\n",
    "            range_num = range_num + 1\n",
    "        print(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/yanhang/Desktop/saliency/my_models/videotrain/animal_alpaca02', '/Users/yanhang/Desktop/saliency/my_models/videotrain/animal_alpaca01']\n"
     ]
    }
   ],
   "source": [
    "_VideoBatch_Generator(VideoName, 10, (1080,720))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros([3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 5)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(np.copy(X[-1, :, :])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "print(np.arange(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    for i in range(10, 0, -1):\n",
    "        yield i\n",
    "y = test()\n",
    "print(y.__next__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid reduction dimension 3 for input with 3 dimensions. for 'Max' (op: 'Max') with input shapes: [2,3,4], [] and with computed input tensors: input[1] = <3>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1658\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1659\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1660\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Invalid reduction dimension 3 for input with 3 dimensions. for 'Max' (op: 'Max') with input shapes: [2,3,4], [] and with computed input tensors: input[1] = <3>.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-0d804bd63850>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_max_v1\u001b[0;34m(input_tensor, axis, keepdims, name, reduction_indices, keep_dims)\u001b[0m\n\u001b[1;32m   1885\u001b[0m   keepdims = deprecation.deprecated_argument_lookup(\"keepdims\", keepdims,\n\u001b[1;32m   1886\u001b[0m                                                     \"keep_dims\", keep_dims)\n\u001b[0;32m-> 1887\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mreduce_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_max\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   1921\u001b[0m       gen_math_ops._max(\n\u001b[1;32m   1922\u001b[0m           \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ReductionDims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m           name=name))\n\u001b[0m\u001b[1;32m   1924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_max\u001b[0;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[1;32m   5413\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   5414\u001b[0m         \u001b[0;34m\"Max\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5415\u001b[0;31m                name=name)\n\u001b[0m\u001b[1;32m   5416\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5417\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    789\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3298\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3299\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3300\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3301\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1821\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1822\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1823\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1660\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1662\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid reduction dimension 3 for input with 3 dimensions. for 'Max' (op: 'Max') with input shapes: [2,3,4], [] and with computed input tensors: input[1] = <3>."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a = np.ones([2, 3, 4])\n",
    "print(a)\n",
    "for i in range(3, 1, -1):\n",
    "    a = tf.reduce_max(a, i)\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
